\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}}
\newlabel{background}{{2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory}{2}}
\newlabel{theory}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definitions, Concurrent Markov Decision Processes}{2}}
\newlabel{definitions-concurrent-markov-decision-processes}{{3.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Random Field Deconstruction Example}{2}}
\newlabel{random-field-deconstruction-example}{{3.1.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The nMDP with transitions from \((i,j)\)only.}}{3}}
\newlabel{fig:figure1}{{1}{3}}
\newlabel{an-nmdp-definition-can-be-described}{{3.1.1.1}{3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.1.1}An nMDP definition can be described}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}An CMDP definition can be described}{3}}
\newlabel{an-cmdp-definition-can-be-described}{{3.1.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Parent (a) and child (b) within the CMDP.}}{4}}
\newlabel{fig:figure2}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Policy Convergence}{4}}
\newlabel{policy-convergence}{{3.1.3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Block diagram of a CMDP.}}{5}}
\newlabel{fig:figure3}{{3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Convergence}{5}}
\newlabel{convergence}{{3.1.4}{5}}
\newlabel{child-mdp-convergence-on-optimal-policy}{{3.1.4.1}{5}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.4.1}Child MDP Convergence on optimal policy}{5}}
\newlabel{parent-mdp-convergence-on-optimal-policy}{{3.1.4.2}{5}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.4.2}Parent MDP Convergence on optimal policy}{5}}
\newlabel{mapping-disjoint-cmdp-policies-onto-an-optimal-nmdp-policy}{{3.1.4.3}{6}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.4.3}Mapping Disjoint CMDP Policies onto an optimal nMDP Policy}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Simulation}{6}}
\newlabel{simulation}{{3.1.5}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Example Derivation of multi-agent robotics Application}{6}}
\newlabel{example-derivation-of-multi-agent-robotics-application}{{3.1.6}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7}Centralized nMDP}{7}}
\newlabel{centralized-nmdp}{{3.1.7}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.8}Task Allocation MDP}{7}}
\newlabel{task-allocation-mdp}{{3.1.8}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.9}Individual Performance MDP}{7}}
\newlabel{individual-performance-mdp}{{3.1.9}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.10}Convergence}{8}}
\newlabel{convergence-1}{{3.1.10}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.11}complete multi-agent CMDP}{9}}
\newlabel{complete-multi-agent-cmdp}{{3.1.11}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.12}}{9}}
\newlabel{section}{{3.1.12}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.13}Convergence}{9}}
\newlabel{convergence-2}{{3.1.13}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}General Reinforcement Learning}{10}}
\newlabel{general-reinforcement-learning}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Learning and Usage of Transitional Knowledge}{10}}
\newlabel{learning-and-usage-of-transitional-knowledge}{{4.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Transferring Knowledge}{12}}
\newlabel{transferring-knowledge}{{4.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Preliminary Deconstruction and Reconstruction }{12}}
\newlabel{preliminary-deconstruction-and-reconstruction}{{4.2.1}{12}}
\newlabel{decision-making-and-gating}{{4.2.1}{12}}
\newlabel{nauxefve-deconstruction-and-reconstruction}{{4.2.1}{12}}
\newlabel{example-nauxefve-deconstruction}{{4.2.1}{12}}
\newlabel{example-nauxefve-reconstruction}{{4.2.1}{13}}
\newlabel{concurrency-and-re-mapping}{{4.2.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Compensation for temporal scale difference}{14}}
\newlabel{compensation-for-temporal-scale-difference}{{4.2.2}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Compensation for action space factorization}{14}}
\newlabel{compensation-for-action-space-factorization}{{4.2.3}{14}}
\newlabel{compensation-for-action-space-factorization-1}{{4.2.3}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}}{14}}
\newlabel{section-1}{{5}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Simulation}{14}}
\newlabel{simulation-1}{{6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Experiment Two}{14}}
\newlabel{experiment-two}{{6.1}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{14}}
\newlabel{results}{{7}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{14}}
\newlabel{conclusion}{{8}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {9}References}{14}}
\newlabel{references}{{9}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Appendix: Convergent Factors Lemma}{14}}
\newlabel{appendix-convergent-factors-lemma}{{10}{14}}
