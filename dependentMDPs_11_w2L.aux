\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}3 Theory}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}3.0 Defintions, Concurrent Markov Decision Processes:}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Random Field Deconstruction Example:}{3}}
\@writefile{toc}{\contentsline {paragraph}{An nMDP definition can be described:}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}An CMDP definition can be described:}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Policy Convergence:}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Convergence}{5}}
\@writefile{toc}{\contentsline {paragraph}{Child MDP Convergence on optimal policy}{5}}
\@writefile{toc}{\contentsline {paragraph}{Parent MDP Convergence on optimal policy}{5}}
\@writefile{toc}{\contentsline {paragraph}{Mapping Disjoint CMDP Policies onto an optimal nMDP Policy}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Simulation}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.6}Example Derivation of multi-agent robotics Application}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7}Centralized nMDP:}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.8}Task Allocation MDP:}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.9}Individual Performance MDP:}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.10}Convergence}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.11}b. complete multi-agent CMDP:}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.12}}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.13}Convergence}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}4.0 General Reinforcement Learning}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}4.1 Learning and Usage of Transitional Knowledge}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}4.2 Transferring Knowledge}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}4.2.1 Preliminary Deconstruction and Reconstruction }{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}4.2.1.1 Decision Making and Gating}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}4.2.1.2 Na\"ive Deconstruction and Reconstruction}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}4.2.1.2.1 Example Na\"ive Deconstruction}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}4.2.1.2.1 Example Na\"ive Reconstruction}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.5}4.2.1.3 Concurrency and re-mapping}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}4.2.1 Compensation for temporal scale difference}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}4.2.2 Compensation for action space factorization}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}4.2.3 Compensation for action space factorization}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {5}}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}4 Simulation}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Experiment Two}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {9}References}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Appendix: Convergent Factors Lemma}{16}}
