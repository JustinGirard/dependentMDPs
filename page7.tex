Although many decomposition strategies are possible, this work presents a specific approach related to parent--child decomposition. This specific decomposition allows for assured policy convergence (see Section~\ref{})

\underline{Definitions}\\

\underline{$M$}\quad
\begin{minipage}[t]{5in}
$S=(S_i/S_k)\times(S_k/S_i)$\\
$A=A_i\cup A_k$\\
$T=P(S\times A\times S)$\\
$R= \text{real, positive, convergent stochastic as $t\to\infty$}$\\
$R(s^\prime,a,s) =R\left( 
\begin{array}{c} s^\prime_i \\ s^\prime_k \end{array},
\begin{array}{c} a_i \\ a_k \end{array},
\begin{array}{c} s_i \\ s_k \end{array}
\right)$
\end{minipage}\\

The system can be broken into the following MDP definitions

\underline{$M_i$ -- Parent}\\

$S_i$ -- a collection of states, $s_i\in S_i$\\
$A_i$ -- a collection of actions, $a_i \in A_i$\\
$\tilde{T}( s^\prime_i | s_i, a_i )$ -- the observed probability of executing action $a_i$ in state $s_i$ and ending up in state $s^\prime_i$\\

\begin{equation*}
\left. \begin{array}{l}
\tilde{T}\\
\tilde{R}
\end{array}\right\}
\text{Covered Pages on BII p12-14}
\end{equation*}

$P(s^\prime_i|s_i, a_i)$ is observed directly and is $\tilde{T}( s^\prime_i | s_i, a_i )$\\

%$R_t\left({}^{s^\prime_i}_{a^\prime_k}|{}_{a_i},{}^{s_i}_{a_k}\right)=R_t\left({}^{s^\prime_i}_{s^\prime_k},{}^{a_i}_{a_k},{}^{s_i}_{s_k}\right)$

\begin{equation*}
\tilde{R}_t\left(  \begin{array}{l} s^\prime_i \\ a^\prime_k \end{array}
\middle| 
\begin{array}{c}  a_i \\ \end{array},
\begin{array}{c} s_i \\ a_k \end{array}
\right) 
=
\tilde{R}_t\left( 
\begin{array}{c} s^\prime_i \\ s^\prime_k \end{array}, \middle|
\begin{array}{c} a_i \\ a_k \end{array},
\begin{array}{c} s_i \\ s_k \end{array}
\right)
=
\tilde{R}( s^\prime | a, s )
\end{equation*}


$S$, $T_i$, $S_k$, $S^\prime_k$ are not directly observable by process $M_i$, by design. Importantly, some facts are known about $a_k$ and $a^\prime_k$ which will be exploited in Section~\ref{}.\\

ii) $a_k=\pi_k(s_k)$\\
iii) $a^\prime_k=\pi_k(s_k)$\\
iiii) $\left(s_k,s^\prime_k\right)$ chosen indirectly by $\pi_k(\cdot)$ in a manner that assuming monotonic increase in reward,
\begin{equation*}
  \text{as\ }t \to \infty \quad E[R_{t+1}(\cdot)] \geq E[R_t(\cdot)]\,.
\end{equation*}
Local convergence of this process on a behavior policy $\pi^*_i(\cdot)$ is assured.

\underline{$M_k$ -- child}\\

$S^\prime_i$ -- all child states, $s_k\in S_k$\\
$a_k\in A_k$\\
$\tilde{T}(s^\prime_k|s_k,a_k)$\\
$R_t(s^\prime_k,a_k,s_k)=R_t\left({}^{s_i}_{s^\prime_k},{}^{a_i}_{a_k},{}^{s_i}_{s_k}\right)$
s.t.\ $s_i$, $s^\prime_i$ are chosen by another process, and
\begin{equation*}
\boxed{A^*}\quad\longrightarrow\quad E[R_{t+1}(\cdot)]\geq E[R_t(\cdot)]
\end{equation*}
It is direct to note that both processes $M_i$ and $M_k$ are guaranteed to converge on locally optimal policies $\pi_i(\cdot)$, $\pi_k(\cdot)$ as the limit of time approaches infinity. 

In following sections next steps are discussed:
\begin{itemize}
\item $\pi^\ast_i \times \pi^\ast_k \to \pi^\ast${\ }utilizing the degenerate-optimal policies (Section~\ref{})
\item $d^{M_i, M_k}_{M}$ -- how to map values while preserving information, avoiding degeneracies (Section~\ref{sec:mapping})
\end{itemize}
