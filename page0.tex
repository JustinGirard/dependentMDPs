 

\section*{page 0: Reconfigurable Reinforcement Learning Networks}

In general, most online optimization problems can be expressed as fully observable Markov Decision Processes (MDPs)
as \( \langle S, A, T, R \rangle \) tuples (State S, Action A, Transitions T, Reward R).

In general, we can express behaviour in this domain as a policy \( \pi: S\rightarrow R\) {\textbf{[? looked like this, but would\ }}  \(\pi:S\rightarrow A \) 
{\textbf{\ make more sense?]}}. Particular attention is given to the optimal strategy.

In prior work the issue of tractability and subsequent decomposition have been articulated.  In this work the subject of learning
and generalizing this decomposition work into a General framework is discussed.

\begin{align*}
\text{\textcircled{A} Theory} & \left\{ 
\begin{array}{ll}
\text{Introduction (4):} & \text{reconfigurable RL introduction \& overview} \\
\text{Mapping (11):} & \text{a generalized set of mapping \& deconstruction operations (parent, child)} \\
\text{Convergence (16):} & \text{parent, child, reward optimization, complexity} \\
\text{Worst Case Performance (23):} & \text{system behaviour with malformed problems}
\end{array}
\right. \\
\text{\textcircled{B} NN paper} & \left\{ 
\begin{array}{ll}
\text{Neural Networks (24):} & \text{RRLN are just feed forward Neural Networks} \\
\text{\(\hookrightarrow\) (N.)} &  \text{\ }
\end{array}
\right.
\end{align*}

{\ }\\

Special topics:\\

 
\noindent\hspace*{1cm}Temporal Difference (A1): how to discover \& change time basis/scale\\
\hspace*{1cm}Transitional Learning (A2): how to re-use and generalize transitional models\\
\hspace*{1cm}Financial Systems (A3): how to use with financial systems\\
\hspace*{1cm}Origins (E1-E4): original examples and sketches\\
\hspace*{1cm}Transitional Encoding (E5-E6): Continuous bnns \textbf{[?]} \& applications
 

 
