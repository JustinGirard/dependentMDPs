 \section*{ Reconfigurable Reinforcement Learning Networks}

In humans, the process of learning is not only driven by the environment and structure of the brain. The development of the brain-structure itself defines what learning may take place; thus the conditions and patterns which direct brain formation are primary and total for the success of learning. As artifical intelligence research continually generates and publishes on novel structures discovered by humans, this work is centered around how the novel structures can be discovered using RLNs. This subject is frequently included in the subject of general intelligence, and is famous for both its philosopical and computational complexity, as well as its difficulty in finding funding. There have been previous works on this subject, such as [Consciousness as a State of Matter] [] []. 

Specifically, this work presents a unification method for online learning (via Reinforcement Learning) and offline learning (via Backpropagation). In the most general sense, this work demonstrates an apporach to the self-structuring of parametric models. First, it is reviewed that Concurrent Markov Decision Processes (CMDPs) can model parametric structure and facilitate optimal behaviour even when subject to large state spaces and generous state uncertainty. Second, it is shown that a variation of CMDPs called Reconfigurable Learning Networks (RLNs) can learn parametric decision networks. RLNs in structure and behaviour turn out to be equilivent to the structure and behaviour of feed-forward neural networks. Lastly, a few empirical examples are demonstrated, beginning with the MINST dataset. Two main contributions are made: First, RLNs can be trained online and offline, using Reinforcement Learning and then Backpropagation; online learning stimulates network growth and adaptation immediately, whereas backpropagation seems to be an ideal phase for network pruning.  Second, an RLN can achie e empirical success even when the reward function for the system is changed dynamically. Thus both a degree of empirical success and general learning have been achieved.

In order for a generally intelligent system to operate, solutions to several open problems need to be solved anaytically and/or heuristically. In this work, we present the related problem categories in the Introduction (Section \ref{sec:intro}), and include background on each area. Second, most of this work is focused around the reconfiguration of existing MDP models, so Section 2, Mapping, includes work on transfer learning and analytical anaysis. Third, we express how convergence of behaviour policies can be preserved despite online RLN restructuring (Section 3). The tradeoff between network structure and computation time  in learning is expressed analytically (Section 4). Lastly, it is shown that RLNs are actually just feed-forward Neural Networks, which adds the ability to use back propagation and other techniques on discovered models (Section 5).

In this work due to the diffiucty of the subject matter initally, models are assumed noiseless and stochastically stable. It is expected that later work will broaden this work by considering state uncertainty, and non-stationary problems.

\section*{Notation}

In general, most online optimization problems can be expressed as fully observable Markov Decision Processes (MDPs)
as \( \langle S, A, T, R,\pi \rangle \) tuples: 

\begin{itemize}  
\item $S \subseteq R^{n}$:  A discrete collection of states.
\item $A \subseteq R^{n}$:  A discrete collection of actions.
\item $T(s^\prime|a,s) \in R $:  A stably stochastic transition function, where $\sum_{s \in S} T(s|a,s') = 1 $
\item $R(s^\prime|a,s) \in R $:  A stable stochastic reward fuction
\item $\pi: S\times A \rightarrow R $:  A non-negative behaviour policy with the general property, $\sum_{a \in A} \pi(s,a) = 1$

\end{itemize}


In general, we can express behaviour in this domain as a policy \( \pi: S\rightarrow R\) {\textbf{[? looked like this, but would\ }}  \(\pi:S\rightarrow A \) 
{\textbf{\ make more sense?]}}. Particular attention is given to the optimal strategy.

In prior work the issue of tractability and subsequent decomposition have been articulated.  In this work the subject of learning
and generalizing this decomposition work into a General framework is discussed.

\begin{align*}
\text{\circled{A} Theory} & \left\{ 
\begin{array}{ll}
\text{Section \ref{sec:intro}: Background \& Introduction} & \text{Background of relevant research \& RLN introduction}\\
\text{Section 2: Mapping} & \text{a generalized set of mapping \& deconstruction operations (parent, child)} \\
\text{Section 3: Convergence (16):} & \text{parent, child, reward optimization, complexity} \\
\text{Section 4: Worst Case Performance (23):} & \text{system behaviour with malformed problems}
\end{array}
\right. \\
\text{\circled{B} NN paper} & \left\{ 
\begin{array}{ll}
\text{Section 5: Neural Networks (24):} & \text{RRLN are just feed forward Neural Networks} \\
\text{\(\hookrightarrow\) (N.)} &  \text{\ }
\end{array}
\right.
\end{align*}

{\ }\\

Special topics:\\

 
\noindent\hspace*{1cm}Temporal Difference (A1): how to discover \& change time basis/scale\\
\hspace*{1cm}Transitional Learning (A2): how to re-use and generalize transitional models\\
\hspace*{1cm}Financial Systems (A3): how to use with financial systems\\
\hspace*{1cm}Origins (E1-E4): original examples and sketches\\
\hspace*{1cm}Transitional Encoding (E5-E6): Continuous Gaussian mixture models \& applications
 

 
